# Scrape Package Design

## Overview

The `scrape` package executes web scraping operations based on configuration files (typically generated by the `generate` package). It implements two main extraction strategies—**nested** and **sequential**—to handle different HTML structures, and supports advanced features like field transformations, date parsing, pagination, and detail page scraping.

## Core Concept

The scraper reads YAML configuration files that specify:
- **What to scrape**: CSS selectors for containers and fields
- **How to extract**: Field types, attributes, transformations
- **How to navigate**: Pagination rules
- **What to validate**: Required fields, filters

It then executes these instructions to produce structured records from unstructured HTML.

## Architecture

### File Structure

- **scrape.go** - Complete scraping implementation: configuration, strategies, field extraction, detail pages

### Key Data Structures

#### `Config`
Top-level configuration for a scraping job:

```go
type Config struct {
    ID       ConfigID                // Unique identifier
    Writer   output.WriterConfig     // Output format configuration
    Scrapers []Scraper               // List of scrapers (list page, detail page, etc.)
    Global   GlobalConfig            // Global settings (user agent, etc.)
    Records  output.Records          // Extracted records
}
```

#### `ConfigID`
Hierarchical identifier for configurations:

```go
type ConfigID struct {
    Slug  string  // Domain/source slug (e.g., "gmail-1844972871065198150")
    ID    string  // Configuration variant (e.g., "n03a", "s05a")
    Field string  // Field name for detail pages (e.g., "Fd1f7685c-href-0")
    SubID string  // Sub-configuration ID (e.g., "n01a")
}

// Example: "gmail-1844972871065198150__n03a_Fd1f7685c-href-0_s01a"
// Means: Gmail email, nested config 03a, detail page from field Fd1f7685c-href-0, sequential sub-config 01a
```

#### `Scraper`
Individual scraper definition:

```go
type Scraper struct {
    Name         string              // Descriptive name
    URL          string              // Target URL or URL pattern
    Strategy     string              // "nested" or "sequential"
    Selector     string              // CSS selector for record container
    Fields       []Field             // Fields to extract
    Paginators   []Paginator         // Pagination configuration
    Validation   *ValidationConfig   // Validation rules
    RenderJs     bool                // Whether to execute JavaScript
}
```

#### `Field`
Field extraction specification:

```go
type Field struct {
    Name             string              // Field name
    Type             string              // "text", "url", "date", "image", "json"
    ElementLocations []ElementLocation   // Where to find this field
    Regex            *RegexConfig        // Regex extraction
    Transform        *TransformConfig    // Value transformation
    DateComponents   []DateComponent     // Date parsing configuration
    Filters          []Filter            // Value filters
    Hide             bool                // Don't include in output
}
```

#### `ElementLocation`
Specifies where and how to extract a value:

```go
type ElementLocation struct {
    Selector  string  // CSS selector (relative to record container)
    Attribute string  // HTML attribute to extract ("" for text content, "href", "src", etc.)
    Fallbacks []ElementLocation  // Alternative locations if primary fails
}
```

## Extraction Strategies

### Nested Strategy

**Use case**: When all fields are descendants of a repeating container element.

**HTML Pattern**:
```html
<div class="event">
  <h2 class="title">Event Title</h2>
  <span class="date">2025-01-15</span>
  <a class="link" href="/register">Register</a>
</div>
<div class="event">
  <h2 class="title">Another Event</h2>
  <span class="date">2025-01-20</span>
  <a class="link" href="/register2">Register</a>
</div>
```

**Configuration**:
```yaml
strategy: nested
selector: div.event  # Repeating container
fields:
  - name: title
    element_locations:
      - selector: h2.title  # Relative to div.event
  - name: date
    element_locations:
      - selector: span.date
  - name: url
    element_locations:
      - selector: a.link
        attribute: href
```

**Execution Flow**:
1. Find all elements matching `div.event` → N selections
2. For each selection:
   - Find `h2.title` within this `div.event`
   - Find `span.date` within this `div.event`
   - Find `a.link[href]` within this `div.event`
   - Combine into one record
3. Return N records

**Implementation**: `GQSelection()` function

### Sequential Strategy

**Use case**: When record data is split across sibling elements rather than nested in a single container.

**HTML Pattern**:
```html
<div class="container">
  <div class="event-info">
    <h2>Event Title 1</h2>
  </div>
  <div class="event-desc">
    <span>2025-01-15</span>
    <a href="/register">Register</a>
  </div>
  <div class="event-info">
    <h2>Event Title 2</h2>
  </div>
  <div class="event-desc">
    <span>2025-01-20</span>
    <a href="/register2">Register</a>
  </div>
</div>
```

**Configuration**:
```yaml
strategy: sequential
selector: div.container  # Parent container
fields:
  - name: title
    element_locations:
      - selector: div.event-info > h2
  - name: date
    element_locations:
      - selector: div.event-desc > span
  - name: url
    element_locations:
      - selector: div.event-desc > a
        attribute: href
```

**Execution Flow**:
1. Find parent element matching `div.container`
2. Get all descendants matching field selectors
3. **Group by position**: Assume fields appear in order
   - First `h2`, first `span`, first `a` → Record 1
   - Second `h2`, second `span`, second `a` → Record 2
4. Handle special cases (date elements that wrap text)
5. Return N records

**Implementation**: `scrapeSequential()` function

**Key Challenge**: Matching fields across different containers

The algorithm uses **date elements as anchors**:
- Date fields tend to appear once per record
- Clone the HTML tree
- Remove non-date elements
- Count how many date containers remain
- This tells us how many records exist
- Build records by extracting Nth occurrence of each field

## Field Extraction

### Text Extraction (`type: text`)

Extracts text content from an element:

```yaml
- name: title
  type: text
  element_locations:
    - selector: h2.title
```

**Implementation**:
```go
func getTextString(e *ElementLocation, sel *fetch.Selection) (string, error) {
    elements := sel.Find(e.Selector)
    if elements.Length() == 0 {
        return "", fmt.Errorf("no elements found for %s", e.Selector)
    }

    var texts []string
    elements.Each(func(_ int, s *goquery.Selection) {
        text := strings.TrimSpace(s.Text())
        if text != "" {
            texts = append(texts, text)
        }
    })

    return strings.Join(texts, FieldPartSeparator), nil  // "\n"
}
```

### URL Extraction (`type: url`)

Extracts and normalizes URLs from href/src attributes:

```yaml
- name: registration_url
  type: url
  element_locations:
    - selector: a.register-link
      attribute: href
```

**Features**:
- Resolves relative URLs using base URL
- Handles base tags in HTML
- Returns absolute URL

### Date Extraction (`type: date`)

Parses dates from text or datetime attributes:

```yaml
- name: event_date
  type: date
  element_locations:
    - selector: time
      attribute: datetime
    - selector: span.date  # Fallback if datetime attribute missing
```

**Features**:
- Uses `phil/datetime` package for flexible parsing
- Handles various date formats (ISO, US, European, etc.)
- Year guessing for dates without years (assumes current or next year)
- Multiple format attempts

**Implementation**:
```go
func extractField(ctx, f *Field, rec, sel, baseURL, baseYear) error {
    // ... get text/attribute value ...

    if f.Type == "date" {
        // Try parsing as date
        dts := datetime.Parse(value)
        if len(dts) > 0 {
            rec[f.Name] = dts[0].Format("2006-01-02")
            return nil
        }

        // Year guessing logic if parse fails
        if baseYear > 0 {
            dts = datetime.ParseWithYear(value, baseYear)
            // ...
        }
    }
}
```

### Image Extraction (`type: image`)

Extracts image URLs from src attributes:

```yaml
- name: poster_image
  type: image
  element_locations:
    - selector: img.poster
      attribute: src
```

Same as URL extraction but specifically for images.

### JSON Extraction (`type: json`)

Extracts structured data from JSON in `<script type="application/ld+json">` tags:

```yaml
- name: price
  type: json
  element_locations:
    - selector: script[type="application/ld+json"]
      attribute: offers.price
```

Uses JSONPath to extract values from embedded JSON.

## Advanced Features

### Fallback Locations

If primary selector fails, try fallbacks:

```yaml
- name: date
  element_locations:
    - selector: time[datetime]
      attribute: datetime
      fallbacks:
        - selector: time  # Try text content if no datetime attribute
        - selector: span.date  # Try different element entirely
```

### Regex Extraction

Extract part of a value using regex:

```yaml
- name: price
  type: text
  regex:
    exp: \$(\d+\.\d{2})
    index: 1  # Capture group 1
  element_locations:
    - selector: span.price
```

**Example**: `"Price: $49.99"` → `"49.99"`

### Value Transformation

Transform extracted values:

```yaml
- name: title
  transform:
    type: lowercase
  element_locations:
    - selector: h2.title
```

**Supported transforms**:
- `lowercase`: Convert to lowercase
- `uppercase`: Convert to uppercase
- `trim`: Remove leading/trailing whitespace
- Custom transformations can be added

### Filters

Filter records based on field values:

```yaml
filters:
  - field: event_type
    operator: equals
    value: "workshop"
```

**Operators**:
- `equals`: Exact match
- `contains`: Substring match
- `regex`: Regular expression match

Records not matching filters are discarded.

### Validation

Ensure records meet requirements:

```yaml
validation:
  requires_cta_selector: a.register-link  # Must have a link
```

Records without required elements are discarded.

## Detail Page Scraping

### Overview

Many sites have **list pages** linking to **detail pages**. The scraper can:
1. Extract URL fields from list page
2. Follow each URL
3. Scrape detail page
4. Merge detail data into list page record

### Configuration

```yaml
scrapers:
  - name: list-page
    url: https://example.com/events
    strategy: nested
    selector: div.event-summary
    fields:
      - name: title
      - name: summary
      - name: detail_url
        type: url

  - name: detail-page
    strategy: nested
    selector: body  # Single page, no repeating container
    fields:
      - name: full_description
      - name: instructor
      - name: price
```

### Execution

```go
func DetailPages(ctx, cache, c *Config, s *Scraper, recs, domain) error {
    // For each record from list page
    for rec in recs:
        // Get URL field value
        detailURL := rec[s.GetDetailPageURLFields()[0].Name]

        // Fetch detail page
        detailDoc := cache.Get(detailURL)

        // Scrape detail page with detail-page scraper
        detailRec := GQDocument(c, detailScrapers[0], detailDoc)

        // Merge detail fields into list record
        for field, value in detailRec:
            rec[field] = value
}
```

## Pagination

### Configuration

```yaml
paginators:
  - selector: a.next-page
    attribute: href
    max_pages: 10
```

### Execution

1. Scrape current page
2. Find next page link using paginator selector
3. Fetch next page
4. Scrape next page
5. Repeat until no more pages or max_pages reached

## Configuration File Format

### Complete Example

```yaml
scrapers:
  - name: "theembodylab-events"
    url: "https://www.theembodylab.com/schedule"
    strategy: nested
    selector: article.eventitem
    render_js: true

    fields:
      - name: title
        type: text
        element_locations:
          - selector: h1.eventitem-title

      - name: event_date
        type: date
        element_locations:
          - selector: time.event-date
            attribute: datetime
          - selector: time.event-date  # Fallback

      - name: start_time
        type: text
        element_locations:
          - selector: time.event-time-localized-start

      - name: description
        type: text
        element_locations:
          - selector: div.sqs-html-content

      - name: registration_url
        type: url
        element_locations:
          - selector: a.eventitem-meta-export-google
            attribute: href

      - name: tags
        type: text
        element_locations:
          - selector: li.eventitem-meta-tags > a

    filters:
      - field: event_date
        operator: not_empty

    validation:
      requires_cta_selector: a.eventitem-meta-export-google

    paginators:
      - selector: a.item-pagination-link--next
        attribute: href
        max_pages: 5
```

## Execution Pipeline

### Main Entry Points

#### `Page()` - Scrape from URL
```go
func Page(ctx, cache, c *Config, s *Scraper, globalConfig, rawDyn, path) (output.Records, error)
```

1. Fetch URL using cache
2. Optionally render JavaScript
3. Convert to GQDocument
4. Call `GQDocument()`

#### `GQDocument()` - Scrape from parsed HTML
```go
func GQDocument(ctx, c *Config, s *Scraper, gqdoc *fetch.Document) (output.Records, error)
```

1. Initialize filters
2. Determine strategy (nested vs sequential)
3. Find all record containers using `s.Selector`
4. For each container:
   - Call `GQSelection()` (nested) or `scrapeSequential()` (sequential)
   - Extract fields
   - Validate record
   - Apply filters
5. Handle pagination if configured
6. Handle detail pages if configured
7. Return records

### Field Extraction Pipeline

For each field in a record:

1. **Find Elements**: Use `sel.Find(f.ElementLocations[0].Selector)`
2. **Extract Value**: Get text or attribute based on `f.Type`
3. **Apply Regex**: If `f.Regex` specified, extract pattern
4. **Transform**: Apply `f.Transform` if specified
5. **Parse**: Parse dates, URLs, etc. based on `f.Type`
6. **Store**: Add to record map: `rec[f.Name] = value`

### Error Handling

- **Element not found**: Try fallback locations, then log warning and continue
- **Parse errors**: Log warning, store raw value
- **Validation failures**: Discard record, log info
- **Filter mismatches**: Discard record silently
- **Fatal errors**: Network failures, invalid config → return error

## Integration with Generate Package

1. **Generate** analyzes HTML → produces `scrape.Config` → writes YAML
2. **Scrape** reads YAML → executes selectors → produces `output.Records`
3. **Records** written to JSON/CSV/database

The two packages are loosely coupled via the YAML configuration format.

## Performance Considerations

- **CSS Selectors**: goquery uses Go's html parser + cascadia (CSS selector engine)
  - Most selectors are O(n) where n = number of elements
  - Complex selectors (`:nth-child`, descendant combinators) are slower
- **Memory**: Large pages (1MB+) and many records (1000+) can use significant memory
  - Each record is a map[string]interface{}
  - Text extraction creates many string copies
- **Caching**: The `fetch.Cache` interface allows implementations to cache:
  - HTTP responses
  - Rendered JavaScript pages
  - Resolved redirects
- **Pagination**: Each page requires a new HTTP request and parse
  - Can be slow for 10+ pages
  - Consider parallel fetching (not currently implemented)

## Testing

Tests use fixture HTML files and expected output records:

```go
func TestScrapePage(t *testing.T) {
    config := scrape.ReadConfig("testdata/config.yml")
    cache := fetch.NewFileCache("testdata/")

    records, err := scrape.Page(ctx, cache, config, &config.Scrapers[0], nil, false, "")

    assert.NoError(t, err)
    assert.Len(t, records, 10)
    assert.Equal(t, "Expected Title", records[0]["title"])
}
```

See `scrape_test.go` for comprehensive examples.

## Common Patterns

### List + Detail Pattern
```yaml
scrapers:
  - name: list
    fields:
      - name: detail_url
        type: url
  - name: detail
    fields:
      - name: full_content
```

### Multi-Format Date Pattern
```yaml
- name: date
  type: date
  element_locations:
    - selector: time[datetime]
      attribute: datetime
    - selector: time
    - selector: meta[property="event:start_time"]
      attribute: content
    - selector: span.date
```

### Conditional Field Pattern
```yaml
- name: price
  element_locations:
    - selector: span.paid-price
    - selector: span.free-event
      # If first fails, tries second
```

### URL Resolution Pattern
```yaml
- name: registration_url
  type: url  # Automatically resolves relative URLs
  element_locations:
    - selector: a.register
      attribute: href
```
